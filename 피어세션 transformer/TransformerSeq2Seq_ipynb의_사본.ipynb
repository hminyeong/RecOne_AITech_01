{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TransformerSeq2Seq.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j65i49dU8Cjs",
        "outputId": "ad82863a-fd6b-456c-e217-9f9aacc6b7c7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_vocab(file_dir):\n",
        "\n",
        "#     with open(file_dir,'r',encoding='utf8') as vocab_file:\n",
        "#         char2idx = {}\n",
        "#         idx2char = {}\n",
        "#         index = 0\n",
        "#         for char in vocab_file:\n",
        "#             char = char.strip()\n",
        "#             char2idx[char] = index\n",
        "#             idx2char[index] = char\n",
        "#             index+=1\n",
        "\n",
        "#     return char2idx, idx2char\n",
        "\n",
        "# char2idx, idx2char = load_vocab(\"/gdrive/MyDrive/colab/13주 실습/vocab.txt\")"
      ],
      "metadata": {
        "id": "5zOo9UVbNtqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(input_sequence[0], output_sequence[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vttaZVMdTKBY",
        "outputId": "16d17060-e235-493e-cfdf-3c4954c0e225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "바 알\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5J_6sPCOTJ5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zVeC5Mf8C-S"
      },
      "source": [
        "from torch.utils.data import (DataLoader, TensorDataset)\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "class TransformerChat(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # 전체 단어(음절) 개수\n",
        "        self.vocab_size = config[\"vocab_size\"]\n",
        "\n",
        "        # 단어(음절) 벡터 크기\n",
        "        self.embedding_size = config['embedding_size']\n",
        "\n",
        "        # Transformer의 Attention Head 개수\n",
        "        # 어텐션 수행시 병렬로 몇개 분할?\n",
        "        self.num_heads = config['num_heads']\n",
        "\n",
        "        # Transformer Encoder의 Layer 수\n",
        "        self.num_encoder_layers = config['num_encoder_layers']\n",
        "\n",
        "        # Transformer Decoder의 Layer 수\n",
        "        self.num_decoder_layers = config['num_decoder_layers']\n",
        "\n",
        "        # 입력 Sequence의 최대 길이\n",
        "        self.max_length = config['max_length']\n",
        "\n",
        "        # Transformer 내부 FNN 크기\n",
        "        self.hidden_size = config['hidden_size']\n",
        "\n",
        "        # Token Embedding Matrix 선언\n",
        "        # embedding vector (행:단어개수, 열:임베딩벡터 사이즈)\n",
        "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_size)\n",
        "\n",
        "        # Transformer Encoder-Decoder 설계(선언)\n",
        "        self.transformer = nn.Transformer(d_model=self.embedding_size, nhead=self.num_heads, num_encoder_layers=self.num_encoder_layers,\n",
        "                                          num_decoder_layers=self.num_decoder_layers, dim_feedforward=self.hidden_size)\n",
        "       \n",
        "        # 입력 길이 L에 대한 (L X L) mask 생성: 이전 토큰들의 정보만을 반영하기 위한 mask\n",
        "        #       [[1, -inf, -inf, -inf],\n",
        "        #        [1,    1, -inf, -inf],\n",
        "        #               ......\n",
        "        #        [1,    1,    1,    1]]\n",
        "        # 이곳을 채우세요.\n",
        "\n",
        "\n",
        "        # 전체 단어 분포로 변환하기 위한 linear 임베딩벡터를 다시 corpus 형식의 벡터로\n",
        "        # 이곳을 채우세요.\n",
        "\n",
        "\n",
        "    def forward(self, enc_inputs, dec_inputs):\n",
        "\n",
        "        # enc_inputs: [batch, seq_len], dec_inputs: [batch, seq_len]\n",
        "        # enc_input_features: [batch, seq_len, emb_size] -> [seq_len, batch, emb_size]\n",
        "        # 이곳을 채우세요.\n",
        "\n",
        "        # dec_input_features: [batch, seq_len, emb_size] -> [seq_len, batch, emb_size]\n",
        "        # 이곳을 채우세요.\n",
        "\n",
        "\n",
        "        # dec_output_features: [seq_len, batch, emb_size]\n",
        "        dec_output_features = self.transformer(src=enc_input_features, tgt=dec_input_features, src_mask = self.mask, tgt_mask = self.mask)\n",
        "\n",
        "        # hypothesis : [seq_len, batch, vocab_size]\n",
        "        hypothesis = self.projection_layer(dec_output_features)\n",
        "\n",
        "        return hypothesis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmYHzEcU8nRp"
      },
      "source": [
        "# 어휘사전(vocabulary) 생성 함수\n",
        "def load_vocab(file_dir):\n",
        "\n",
        "    with open(file_dir,'r',encoding='utf8') as vocab_file:\n",
        "        char2idx = {}\n",
        "        idx2char = {}\n",
        "        index = 0\n",
        "        for char in vocab_file:\n",
        "            char = char.strip()\n",
        "            # key는 문자, value는 idx\n",
        "            char2idx[char] = index\n",
        "            # key는 idx, value는 문자\n",
        "            idx2char[index] = char\n",
        "            index+=1\n",
        "\n",
        "    return char2idx, idx2char\n",
        "\n",
        "# 문자 입력열을 인덱스로 변환하는 함수\n",
        "def convert_data2feature(config, input_sequence, char2idx, decoder_input=False):\n",
        "\n",
        "    # 고정 길이 벡터 생성\n",
        "    input_features = np.zeros(config[\"max_length\"], dtype=np.int)\n",
        "\n",
        "    if decoder_input:\n",
        "        # Decoder Input은 Target Sequence에서 Right Shift\n",
        "        # Target Sequence :         [\"안\",\"녕\",\"하\",\"세\",\"요\", \"</S>\" ]\n",
        "        # Decoder Input Sequence :  [\"<S>\", \"안\",\"녕\",\"하\",\"세\",\"요\"]\n",
        "        # 이곳을 채우세요.\n",
        "        input_sequence = \" \".join([\"<S>\"] + input_sequence.split()[:-1])\n",
        "\n",
        "    for idx,token in enumerate(input_sequence.split()):\n",
        "        if token in char2idx.keys():\n",
        "            input_features[idx] = char2idx[token]\n",
        "            # 있으면 인덱스형태로 바꿔\n",
        "        else:\n",
        "            input_features[idx] = char2idx['<UNK>']\n",
        "            # 없으면 UNK\n",
        "\n",
        "    return input_features\n",
        "\n",
        "# 데이터 읽기 함수\n",
        "def load_dataset(config):\n",
        "\n",
        "    # 어휘사전 읽어오기\n",
        "    char2idx, idx2char = load_vocab(config['vocab_file'])\n",
        "\n",
        "    file_dir = config['train_file']\n",
        "    data_file = open(file_dir,'r',encoding='utf8').readlines()\n",
        "    # 문장들\n",
        "\n",
        "    # 데이터를 저장하기 위한 리스트 생성\n",
        "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
        "\n",
        "    for line in tqdm(data_file):\n",
        "\n",
        "        line = line.strip().split('\\t')\n",
        "        # 입력문장\n",
        "        input_sequence = line[0]\n",
        "        # 출력문장\n",
        "        output_sequence = line[1]\n",
        "                          # 입력문장을 받아서 코퍼스의 인덱스화\n",
        "        enc_inputs.append(convert_data2feature(config, input_sequence, char2idx))\n",
        "        \n",
        "        dec_inputs.append(convert_data2feature(config, output_sequence, char2idx, True))\n",
        "        dec_outputs.append(convert_data2feature(config, output_sequence, char2idx))\n",
        "\n",
        "    # 전체 데이터를 저장하고 있는 리스트를 텐서 형태로 변환\n",
        "    enc_inputs = torch.tensor(enc_inputs, dtype=torch.long)\n",
        "    dec_inputs = torch.tensor(dec_inputs, dtype=torch.long)\n",
        "    dec_outputs = torch.tensor(dec_outputs, dtype=torch.long)\n",
        "\n",
        "    return enc_inputs, dec_inputs, dec_outputs, char2idx, idx2char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ-bIKnw-1D8"
      },
      "source": [
        "# 텐서를 리스트로 변환하는 함수\n",
        "def tensor2list(input_tensor):\n",
        "    return input_tensor.cpu().detach().numpy().tolist()\n",
        "\n",
        "def do_test(config, model, word2idx, idx2word, input_sequence=\"오늘 약속있으세요?\"):\n",
        "\n",
        "    # 평가 모드 셋팅\n",
        "    model.eval()\n",
        "\n",
        "    # 입력된 문자열의 음절을 공백 단위 토큰으로 변환. 공백은 <SP>로 변환: \"오늘 약속\" -> \"오 늘 <SP> 약 속\"\n",
        "    input_sequence = \" \".join([e if e != \" \" else \"<SP>\" for e in input_sequence])\n",
        "\n",
        "    # 텐서 변환: [1, seq_len]\n",
        "    enc_inputs = torch.tensor([convert_data2feature(config, input_sequence, word2idx)], dtype=torch.long).cuda()\n",
        "    \n",
        "    # input_ids : [1, seq_len] -> 첫번째 디코더 입력 \"<S>\" 만들기\n",
        "    dec_inputs = torch.tensor([convert_data2feature(config, \"\", word2idx, True)], dtype=torch.long).cuda()\n",
        "    \n",
        "    # 시스템 응답 문자열 초기화\n",
        "    response = ''\n",
        "\n",
        "    # 최대 입력 길이 만큼 Decoding Loop\n",
        "    for decoding_step in range(config['max_length']-1):\n",
        "\n",
        "        # dec_outputs: [vocab_size]\n",
        "        dec_outputs = model(enc_inputs, dec_inputs)[decoding_step, 0, :]\n",
        "        # 가장 큰 출력을 갖는 인덱스 얻어오기\n",
        "        dec_output_idx = np.argmax(tensor2list(dec_outputs))\n",
        "\n",
        "        # 생성된 토큰은 dec_inputs에 추가 (첫번째 차원은 배치)\n",
        "        dec_inputs[0][decoding_step+1] = dec_output_idx\n",
        "\n",
        "        # </S> 심볼 생성 시, Decoding 종료\n",
        "        if idx2word[dec_output_idx] == \"</S>\":\n",
        "            break\n",
        "\n",
        "        # 생성 토큰 추가\n",
        "        response += idx2word[dec_output_idx]\n",
        "    \n",
        "    # <SP>를 공백으로 변환한 후 응답 문자열 출력\n",
        "    print(response.replace(\"<SP>\", \" \"))\n",
        "\n",
        "def test(config):\n",
        "\n",
        "    # 어휘사전 읽어오기\n",
        "    word2idx, idx2word = load_vocab(config['vocab_file'])\n",
        "\n",
        "    # Transformer Seq2Seq 모델 객체 생성\n",
        "    model = TransformerChat(config).cuda()\n",
        "\n",
        "    # 학습한 모델 파일로부터 가중치 불러옴\n",
        "    model.load_state_dict(torch.load(os.path.join(config[\"output_dir\"], config[\"trained_model_name\"])))\n",
        "\n",
        "    while(True):\n",
        "        input_sequence = input(\"문장을 입력하세요. (종료는 exit을 입력하세요.) : \")\n",
        "        if input_sequence == 'exit':\n",
        "            break\n",
        "        do_test(config, model, word2idx, idx2word, input_sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow01KJjz-416"
      },
      "source": [
        "def train(config):\n",
        "\n",
        "    # Transformer Seq2Seq 모델 객체 생성\n",
        "    model = TransformerChat(config).cuda()\n",
        "\n",
        "    # 데이터 읽기\n",
        "    enc_inputs, dec_inputs, dec_outputs, word2idx, idx2word = load_dataset(config)\n",
        "\n",
        "    # TensorDataset/DataLoader를 통해 배치(batch) 단위로 데이터를 나누고 셔플(shuffle)\n",
        "    train_features = TensorDataset(enc_inputs, dec_inputs, dec_outputs)\n",
        "    train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n",
        "\n",
        "    # 크로스엔트로피 손실 함수\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 옵티마이저 함수 지정\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learn_rate\"])\n",
        "\n",
        "    for epoch in range(config[\"epoch\"] + 1):\n",
        "\n",
        "        for (step, batch) in enumerate(train_dataloader):\n",
        "\n",
        "            # 학습 모드 셋팅\n",
        "            model.train()\n",
        "          \n",
        "            # batch = (enc_inputs[step], dec_inputs[step], dec_outputs)*batch_size\n",
        "            # .cuda()를 통해 메모리에 업로드\n",
        "            batch = tuple(t.cuda() for t in batch)\n",
        "\n",
        "            # 역전파 변화도 초기화\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            enc_inputs, dec_inputs, dec_outputs = batch\n",
        "\n",
        "            # hypothesis: [seq_len, batch, vocab_size] -> [seq_len*batch, vocab_size]\n",
        "            # 이곳을 채우세요.\n",
        "\n",
        "\n",
        "            # labels: [batch, seq_len] -> [seq_len, batch] -> [seq_len(max_length)*batch]\n",
        "            labels = dec_outputs.transpose(0, 1)\n",
        "            labels = labels.reshape(config[\"max_length\"]*dec_inputs.size(0))\n",
        "\n",
        "            # 비용 계산 및 역전파 수행: cross_entopy 내부에서 labels를 원핫벡터로 변환 (골드레이블은 항상 1차원으로 입력)\n",
        "            loss = loss_func(hypothesis, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # 200 배치마다 중간 결과 출력\n",
        "            if (step+1)% 200 == 0:\n",
        "                print(\"Current Step : {0:d} / {1:d}\\tCurrent Loss : {2:f}\".format(step+1, int(len(enc_inputs) / config['batch_size']), loss.item()))\n",
        "                # 생성 문장을 확인하기 위한 함수 호출\n",
        "                # do_test(config, model, word2idx, idx2word)\n",
        "\n",
        "        # 에폭마다 가중치 저장\n",
        "        torch.save(model.state_dict(), os.path.join(config[\"output_dir\"], \"epoch_{0:d}.pt\".format(epoch)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "n5lI4zOzobMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fk1BLZ-VrHe",
        "outputId": "0f3aeeab-021a-4923-9131-a5667c9e6d5d"
      },
      "source": [
        "if(__name__==\"__main__\"):\n",
        "\n",
        "    root_dir = \"//gdrive/MyDrive/colab/13주 실습/\"\n",
        "    output_dir = os.path.join(root_dir, \"output\")\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    config = {\"mode\": \"train\",\n",
        "              \"vocab_file\": os.path.join(root_dir, \"vocab.txt\"),\n",
        "              \"train_file\": os.path.join(root_dir, \"train.txt\"),\n",
        "              \"trained_model_name\":\"epoch_{}.pt\".format(10),\n",
        "              \"output_dir\":output_dir,\n",
        "              \"epoch\": 10,\n",
        "              \"learn_rate\":0.00005,\n",
        "              \"num_encoder_layers\": 6,\n",
        "              \"num_decoder_layers\": 6,\n",
        "              \"num_heads\": 4,\n",
        "              \"max_length\": 20,\n",
        "              \"batch_size\": 128,\n",
        "              \"embedding_size\": 256,\n",
        "              \"hidden_size\": 512,\n",
        "              \"vocab_size\": 4427\n",
        "            }\n",
        "\n",
        "    if(config[\"mode\"] == \"train\"):\n",
        "      train(config)\n",
        "    else:\n",
        "      test(config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 547958/547958 [00:11<00:00, 47805.15it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:61: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Step : 200 / 1\tCurrent Loss : 2.680416\n",
            "Current Step : 400 / 1\tCurrent Loss : 2.285882\n",
            "Current Step : 600 / 1\tCurrent Loss : 2.175237\n",
            "Current Step : 800 / 1\tCurrent Loss : 2.087821\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.995788\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.950016\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.921225\n",
            "Current Step : 1600 / 1\tCurrent Loss : 2.009286\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.866276\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.923840\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.716198\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.872773\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.740725\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.877581\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.739158\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.761191\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.770121\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.742987\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.647536\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.638652\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.781530\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.613224\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.694342\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.714745\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.825738\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.787119\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.763044\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.576001\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.641705\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.642183\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.769452\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.577312\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.659484\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.603395\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.796693\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.706377\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.674672\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.677451\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.754497\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.680505\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.594319\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.677139\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.568331\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.604937\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.660445\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.660910\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.550061\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.763803\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.586019\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.537131\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.696624\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.605015\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.718486\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.735691\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.615631\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.738352\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.636252\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.639182\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.515146\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.632266\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.725174\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.499698\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.613131\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.650857\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.524928\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.509774\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.727525\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.557826\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.711863\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.644526\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.535123\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.755340\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.604420\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.629294\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.619843\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.569202\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.660246\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.653543\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.520895\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.670389\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.638643\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.643634\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.688853\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.639210\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.490610\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.475470\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.643162\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.467228\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.521768\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.539608\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.504212\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.545796\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.495497\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.528660\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.540516\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.587776\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.581478\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.516146\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.566461\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.560294\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.642633\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.550822\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.533359\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.621580\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.429986\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.543435\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.581626\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.509785\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.710980\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.448813\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.592411\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.535320\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.564604\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.666212\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.520481\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.572761\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.559953\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.558025\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.533854\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.558455\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.568661\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.636068\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.638817\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.445900\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.489453\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.521720\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.478690\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.564804\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.439050\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.467414\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.522616\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.590801\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.510365\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.619419\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.509621\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.515091\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.546919\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.569232\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.605012\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.598546\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.511949\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.504377\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.439489\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.533365\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.470873\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.515605\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.498750\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.657058\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.570403\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.619534\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.573511\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.498053\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.656493\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.681733\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.461182\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.420989\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.540409\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.531922\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.566679\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.459941\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.496665\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.469526\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.509680\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.537509\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.641649\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.525404\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.629643\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.584137\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.397309\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.536524\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.523050\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.552924\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.482361\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.563255\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.584169\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.557097\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.589159\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.462028\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.478240\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.610543\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.545254\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.522078\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.535665\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.531058\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.512983\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.490729\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.538202\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.523481\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.345068\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.556941\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.490171\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.480042\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.484170\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.457148\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.518610\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.417054\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.436730\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.343856\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.514934\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.542485\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.469414\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.468032\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.654402\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.501729\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.529355\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.598115\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.594609\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.435659\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.409805\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.579612\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.502240\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.454440\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.412789\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.600017\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.492433\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.501851\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.399572\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.388189\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.492015\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.505906\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.520811\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.497992\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.508226\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.545644\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.482508\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.463212\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.421117\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.507674\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.495860\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.565894\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.450081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIhLXar0Ta-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b96a9f6-1a31-4723-c3d0-0890af953f21"
      },
      "source": [
        "if(__name__==\"__main__\"):\n",
        "\n",
        "    root_dir = \"//gdrive/MyDrive/colab/13주 실습/\"\n",
        "    output_dir = os.path.join(root_dir, \"output\")\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    config = {\"mode\": \"test\",\n",
        "              \"vocab_file\": os.path.join(root_dir, \"vocab.txt\"),\n",
        "              \"train_file\": os.path.join(root_dir, \"train.txt\"),\n",
        "              \"trained_model_name\":\"epoch_{}.pt\".format(10),\n",
        "              \"output_dir\":output_dir,\n",
        "              \"epoch\": 10,\n",
        "              \"learn_rate\":0.00005,\n",
        "              \"num_encoder_layers\": 6,\n",
        "              \"num_decoder_layers\": 6,\n",
        "              \"num_heads\": 4,\n",
        "              \"max_length\": 20,\n",
        "              \"batch_size\": 128,\n",
        "              \"embedding_size\": 256,\n",
        "              \"hidden_size\": 512,\n",
        "              \"vocab_size\": 4427\n",
        "            }\n",
        "\n",
        "    if(config[\"mode\"] == \"train\"):\n",
        "        train(config)\n",
        "    else:\n",
        "        test(config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문장을 입력하세요. (종료는 exit을 입력하세요.) : 안녕하세요\n",
            "안녕하세요\n",
            "문장을 입력하세요. (종료는 exit을 입력하세요.) : 요즘 날씨가 춥죠?\n",
            "네 맞아요 ㅎㅎ\n",
            "문장을 입력하세요. (종료는 exit을 입력하세요.) : 저녁 드셨어요?\n",
            "아니요 ㅎㅎ\n",
            "문장을 입력하세요. (종료는 exit을 입력하세요.) : 어떤 음식 좋아하세요?\n",
            "저는 저는 닭갈비 먹어요\n",
            "문장을 입력하세요. (종료는 exit을 입력하세요.) : 매운 음식 좋아하시나봐요?\n",
            "저는 그냥 먹어요\n",
            "문장을 입력하세요. (종료는 exit을 입력하세요.) : 떡볶이 좋아해요?\n",
            "아니요 ㅎㅎ\n",
            "문장을 입력하세요. (종료는 exit을 입력하세요.) : 치킨은 좋아해요?\n",
            "아니요 ㅎㅎ\n",
            "문장을 입력하세요. (종료는 exit을 입력하세요.) : 집에 가고 싶으신가봐요?\n",
            "네 저는 아직 안가요\n",
            "문장을 입력하세요. (종료는 exit을 입력하세요.) : 한국어를 대충 배우셨나봐요?\n",
            "아니요\n",
            "문장을 입력하세요. (종료는 exit을 입력하세요.) : 기분 나쁘셨어요?\n",
            "네 ㅎㅎㅎ\n",
            "문장을 입력하세요. (종료는 exit을 입력하세요.) : 죄송합니다.\n",
            "아 그렇구나\n",
            "문장을 입력하세요. (종료는 exit을 입력하세요.) : exit\n"
          ]
        }
      ]
    }
  ]
}